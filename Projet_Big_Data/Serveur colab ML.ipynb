{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow pyngrok\n",
        "!pip install vaderSentiment\n",
        "!pip install fastapi uvicorn\n"
      ],
      "metadata": {
        "id": "3QeO2fGqbYte",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e47ca3c-ef74-4d79-9f85-d7417a0543ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-2.19.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting mlflow-skinny==2.19.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.19.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<19,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (17.0.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.6.0)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.36)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.0)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.19.0->mlflow)\n",
            "  Downloading databricks_sdk-0.40.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.43)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (8.5.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (4.25.5)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.2.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.27.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (4.0.11)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.2.15)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.50b0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2024.12.14)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.17.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (5.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.6.1)\n",
            "Downloading mlflow-2.19.0-py3-none-any.whl (27.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.19.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.40.0-py3-none-any.whl (629 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m629.7/629.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, Mako, gunicorn, graphql-core, graphql-relay, docker, alembic, graphene, databricks-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed Mako-1.3.8 alembic-1.14.0 databricks-sdk-0.40.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.5 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.19.0 mlflow-skinny-2.19.0 pyngrok-7.2.3\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.12.14)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.10.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.6 starlette-0.41.3 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy5XSysRDi5S"
      },
      "outputs": [],
      "source": [
        "'''Adding a Model to MLflow'''\n",
        "import mlflow\n",
        "import mlflow.keras\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "def log_model(model):\n",
        "      # Set MLflow tracking URI to Google Drive or a persistent directory\n",
        "      mlflow.set_tracking_uri(\"/content/mlruns\")\n",
        "\n",
        "      # Evaluate the model and capture metrics\n",
        "      #loss= model.evaluate(X_test, y_test, verbose=0)  # Add other metrics if available\n",
        "      #matrics are added in the compiler\n",
        "\n",
        "      # Start a new MLflow run\n",
        "      with mlflow.start_run() as run:\n",
        "          # Add metadata (timestamp)\n",
        "          timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "          mlflow.log_param(\"run_timestamp\", timestamp)\n",
        "\n",
        "          # Log metrics\n",
        "          #mlflow.log_metric(\"loss\", loss)\n",
        "          #mlflow.log_metric(\"accuracy\", accuracy)\n",
        "\n",
        "          # Log the model with a unique name\n",
        "          model_name = f\"model\"\n",
        "          mlflow.keras.log_model(model, artifact_path=model_name)\n",
        "\n",
        "          # Print the run ID and model name\n",
        "          print(f\"Model logged with name: {model_name}\")\n",
        "          print(f\"Run ID: {run.info.run_id}\")\n",
        "          #print(f\"Loss: {loss}\")#, Accuracy: {accuracy}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Utilisation de Vader pour le sentiment score'''\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "def sentiment_scores(tweet):\n",
        "      # Initialize the SentimentIntensityAnalyzer\n",
        "      sia = SentimentIntensityAnalyzer()\n",
        "      #print(sia.polarity_scores(tweet))\n",
        "      return sia.polarity_scores(tweet)['compound']\n",
        "\n",
        "print(sentiment_scores(\"I don't know what to say, it just bad\"))\n"
      ],
      "metadata": {
        "id": "MQtPeZqaGgrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e89aa752-aef5-4147-fee4-3d13988e9794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.5423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Serveur d'Entrainement et de Prédiction'''\n",
        "from fastapi import FastAPI\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.layers import Layer, Input, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.utils import get_custom_objects\n",
        "from fastapi import FastAPI, HTTPException\n",
        "import tensorflow as tf\n",
        "from io import StringIO\n",
        "import os\n",
        "import mlflow\n",
        "import mlflow.keras\n",
        "from mlflow.tracking import MlflowClient\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "# Initialize the model and scaler to avoid reloading it for each prediction\n",
        "from tensorflow.keras.saving import register_keras_serializable\n",
        "\n",
        "from keras.layers import Layer, Dense\n",
        "import tensorflow as tf\n",
        "from keras.saving import register_keras_serializable\n",
        "\n",
        "@register_keras_serializable(package='Custom')\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        # Ensure the parent class handles all standard arguments like `trainable`\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "        self.W = Dense(50, use_bias=False)\n",
        "        self.V = Dense(1, use_bias=False)\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        # Compute attention scores\n",
        "        score = self.V(tf.nn.tanh(self.W(hidden_states)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # Compute the context vector\n",
        "        context_vector = tf.reduce_sum(attention_weights * hidden_states, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        # Serialize the layer properly\n",
        "        config = super(Attention, self).get_config()\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Ensure the `trainable` argument is handled correctly\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "def sentiment_scores(tweet):\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    return sid.polarity_scores(tweet)['compound']\n",
        "    # Apply the sentiment_scores function to each tweet\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "def map_sentiment(score):\n",
        "    if score > 0.05:  # Slightly positive or higher\n",
        "        return \"positive\"\n",
        "    elif score < -0.05:  # Slightly negative or lower\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"  # Neutral sentiment\n",
        "\n",
        "def preprocess_data(social_data, stock_data):\n",
        "    # Preprocess the social data and stock data\n",
        "    try:\n",
        "        # Compute sentiment scores\n",
        "        social_data['sentiment_scores'] = social_data['tweet'].apply(sentiment_scores)\n",
        "        social_data['sentiment'] = social_data['sentiment_scores'].apply(map_sentiment)\n",
        "\n",
        "\n",
        "        social_data['time'] = pd.to_datetime(social_data['time'], unit='ms')\n",
        "        stock_data['time'] = pd.to_datetime(stock_data['time'], unit='ms')\n",
        "\n",
        "\n",
        "        # Sort both datasets by time\n",
        "        social_data = social_data.sort_values(by='time')\n",
        "        stock_data = stock_data.sort_values(by='time')\n",
        "\n",
        "\n",
        "        # Merge using `merge_asof` with a time tolerance (e.g., 10 day)\n",
        "        data = pd.merge_asof(stock_data, social_data, on='time', tolerance=pd.Timedelta('60D'), direction='nearest')\n",
        "\n",
        "        data['followers'] = pd.to_numeric(data['followers'], errors='coerce')  # Converts non-numeric to NaN\n",
        "        data['sentiment_scores'] = pd.to_numeric(data['sentiment_scores'], errors='coerce')\n",
        "        data['stock_price'] = data['stock_price'].apply(lambda x: round(x, 3) if np.isfinite(x) else x)\n",
        "        data['followers'] = data['followers'].apply(lambda x: round(x, 3) if np.isfinite(x) else x)\n",
        "        data = data.dropna()\n",
        "        print(\"The data\")\n",
        "        print(data)\n",
        "\n",
        "        if data.empty:\n",
        "            raise ValueError(\"Merged data is empty. Ensure 'time' columns have matching values in both datasets.\")\n",
        "\n",
        "        # Add weighted sentiment feature\n",
        "        data['weighted_sentiment'] = data['followers'] * data['sentiment_scores']\n",
        "\n",
        "        # Select features and target\n",
        "        features = ['weighted_sentiment']\n",
        "        target = 'stock_price'\n",
        "\n",
        "        # Check if required columns exist\n",
        "        missing_columns = [col for col in features + [target] if col not in data.columns]\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Missing columns in data: {missing_columns}\")\n",
        "\n",
        "        # Check if data is non-empty for scaling\n",
        "        if data[features + [target]].shape[0] == 0:\n",
        "            raise ValueError(\"No samples available for scaling.\")\n",
        "\n",
        "        # Normalize features and target\n",
        "        data_complete = data.copy()\n",
        "        scaler = MinMaxScaler()\n",
        "        data[features + [target]] = scaler.fit_transform(data[features + [target]])\n",
        "        data_complete = data_complete.to_dict(orient='records')\n",
        "        return data, features, target, data_complete\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during preprocessing: {e}\")\n",
        "        raise\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    print(\"create_sequence\")\n",
        "    print(data)\n",
        "    print(seq_length)\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length, :-1])  # Features (weighted_sentiment)\n",
        "        y.append(data[i+seq_length, -1])    # Target (stock_price)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def build_and_train_model(X_train, y_train):\n",
        "    print(X_train)\n",
        "    print(y_train)\n",
        "    # Build the LSTM model with attention mechanism\n",
        "    #X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # Reshape for LSTM input\n",
        "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))  # (seq_length, 1)\n",
        "    lstm_out = LSTM(50, activation='relu', return_sequences=True)(inputs)\n",
        "    lstm_out = Dropout(0.2)(lstm_out)\n",
        "\n",
        "    # Apply the custom attention layer\n",
        "    context_vector, attention_weights = Attention()(lstm_out)\n",
        "\n",
        "    # Dense layers\n",
        "    dense_out = Dense(64, activation='relu')(context_vector)\n",
        "    dense_out = Dropout(0.2)(dense_out)\n",
        "    output = Dense(1)(dense_out)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=32)#, validation_data=(X_test, y_test)\n",
        "    return model\n",
        "\n",
        "@app.post(\"/train_model\")\n",
        "async def train_model(request: dict):\n",
        "    # Deserialize the JSON back to DataFrame\n",
        "    print(request)\n",
        "    # Extract stock_data and social_data directly from the request dictionary\n",
        "    stock_data = pd.DataFrame(request['stock_data']['data'], columns=request['stock_data']['columns'])\n",
        "    social_data = pd.DataFrame(request['social_data']['data'], columns=request['social_data']['columns'])\n",
        "\n",
        "    # Preprocess data\n",
        "    data, features, target, data_complete = preprocess_data(social_data, stock_data)\n",
        "\n",
        "    # Create sequences for LSTM\n",
        "    seq_length = 5  # Number of timesteps\n",
        "    data_array = data[features + [target]].values\n",
        "    X, y = create_sequences(data_array, seq_length)\n",
        "\n",
        "    # Train-test split\n",
        "    #split = int(0.8 * len(X))\n",
        "    #X_train, X_test = X[:split], X[split:]\n",
        "    #y_train, y_test = y[:split], y[split:]\n",
        "    print(\"X\")\n",
        "    print(X)\n",
        "    print(y)\n",
        "    X_train = X\n",
        "    y_train = y\n",
        "    get_custom_objects().update({\"Attention\": Attention})\n",
        "    model = build_and_train_model(X_train, y_train)#, X_test, y_test)\n",
        "    log_model(model)\n",
        "    print(data_complete)\n",
        "    return {\"message\": \"Model trained successfully!\", \"complete_data\" : data_complete}\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(request: dict):\n",
        "    #try:\n",
        "        # Process input data\n",
        "        social_data = pd.DataFrame(request['social_data']['data'], columns=request['social_data']['columns'])\n",
        "        print('from source')\n",
        "        print(social_data['time'])\n",
        "        social_data['time'] = pd.to_datetime(social_data['time'], unit='ms')\n",
        "\n",
        "        social_data = social_data.sort_values(by='time')\n",
        "\n",
        "        social_data['sentiment_scores'] = social_data['tweet'].apply(sentiment_scores)\n",
        "        social_data['sentiment'] = social_data['sentiment_scores'].apply(map_sentiment)\n",
        "        social_data['followers'] = pd.to_numeric(social_data['followers'], errors='coerce')  # Converts non-numeric to NaN\n",
        "        social_data['sentiment_scores'] = pd.to_numeric(social_data['sentiment_scores'], errors='coerce')\n",
        "        social_data['followers'] = social_data['followers'].apply(lambda x: round(x, 3) if np.isfinite(x) else x)\n",
        "        social_data = social_data.dropna()\n",
        "        social_data['weighted_sentiment'] = social_data['followers'] * social_data['sentiment_scores']\n",
        "\n",
        "        # Normalize the data\n",
        "        scaler = MinMaxScaler()\n",
        "        social_data[['weighted_sentiment']] = scaler.fit_transform(social_data[['weighted_sentiment']])\n",
        "\n",
        "        # Prepare input for prediction\n",
        "        last_30_days = social_data[-5:]\n",
        "\n",
        "        # Ensure you are selecting the last 5 values from the 'weighted_sentiment' column\n",
        "        X_pred = last_30_days[['weighted_sentiment']].values[-5:].reshape(1, 5, 1)\n",
        "\n",
        "        # If the sequence length exceeds 5, retain only the last 5 time steps\n",
        "        #if X_pred.shape[1] > 5:\n",
        "         #   X_pred = X_pred[:, -5:, :]\n",
        "\n",
        "        # Load the latest model from MLflow\n",
        "        mlflow.set_tracking_uri(\"/content/mlruns\")\n",
        "        get_custom_objects().update({\"Attention\": Attention})\n",
        "        client = MlflowClient()\n",
        "        runs = client.search_runs(experiment_ids=[\"0\"], order_by=[\"start_time DESC\"], max_results=1)\n",
        "\n",
        "        if not runs:\n",
        "            raise HTTPException(status_code=500, detail=\"No MLflow runs found.\")\n",
        "\n",
        "        latest_run = runs[0]\n",
        "        model_uri = f\"runs:/{latest_run.info.run_id}/model\"\n",
        "        print(model_uri)\n",
        "        model = mlflow.keras.load_model(model_uri, custom_objects={'Attention': Attention})\n",
        "\n",
        "        # Predict for the next 7 days\n",
        "        predictions = []\n",
        "        for i in range(7):\n",
        "            print(f\"Iteration {i}\")\n",
        "            prediction = model.predict(X_pred)\n",
        "\n",
        "            # Inverse scale the prediction\n",
        "            prediction_original = scaler.inverse_transform(\n",
        "                np.hstack([np.zeros((len(prediction), 1)), prediction])\n",
        "            )[:, -1]\n",
        "\n",
        "            # Append the single prediction to the list\n",
        "            predictions.append(float(prediction_original[0]))\n",
        "\n",
        "            # Update X_pred with the new prediction\n",
        "            new_data = np.array([[prediction_original[0]]])\n",
        "            X_pred = np.append(X_pred[:, 1:, :], new_data.reshape(1, 1, 1), axis=1)\n",
        "\n",
        "\n",
        "\n",
        "        # Generate prediction dates\n",
        "        print(social_data['time'])\n",
        "        last_date = social_data['time'].iloc[-1]\n",
        "        print(last_date)\n",
        "        prediction_dates = [last_date + timedelta(days=i) for i in range(1, 8)]\n",
        "\n",
        "        # Prepare response\n",
        "        result = [{\"date\": prediction_dates[i].strftime('%Y-%m-%d'), \"predicted_stock_price\": predictions[i]}\n",
        "                  for i in range(len(predictions))]\n",
        "\n",
        "        return {\"predictions\": result}\n",
        "\n"
      ],
      "metadata": {
        "id": "0wD0SKYLX3T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''configuration Ngrok'''\n",
        "# Install necessary packages: aiohttp for async subprocess execution and pyngrok for Ngrok integration\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Set LD_LIBRARY_PATH to prioritize system NVIDIA libraries over built-in ones\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "# Define a helper function to run commands synchronously\n",
        "def run(cmd):\n",
        "    print('>>> starting', *cmd)\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # Process and print the output and error streams\n",
        "    stdout, stderr = process.communicate()\n",
        "\n",
        "    if stdout:\n",
        "        print(stdout.decode('utf-8'))\n",
        "    if stderr:\n",
        "        print(stderr.decode('utf-8'))\n",
        "\n",
        "# Replace this with your Ngrok authentication token\n",
        "NGROK_AUTH_TOKEN = \"2rPSxjtzWTP2vtpDDbK9AQf4VNy_6sdRogw7FGiJPcqgdQ2Pk\"\n",
        "\n",
        "# Authenticate with Ngrok using the token\n",
        "run(['ngrok', 'config', 'add-authtoken', NGROK_AUTH_TOKEN])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb6q-ULhyG04",
        "outputId": "2cef2c19-b47a-4d88-b707-4e9bdeabc630"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> starting ngrok config add-authtoken 2rPSxjtzWTP2vtpDDbK9AQf4VNy_6sdRogw7FGiJPcqgdQ2Pk\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Création Ngrok tunnel '''\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "#!ngrok config add-authtoken 2rPSxjtzWTP2vtpDDbK9AQf4VNy_6sdRogw7FGiJPcqgdQ2Pk\n",
        "# Required to run FastAPI in Colab\n",
        "nest_asyncio.apply()\n",
        "# Expose the server\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "# Start the server\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG6kq5YuyU7H",
        "outputId": "79688f6f-a6e8-47a7-cd38-df9d0a081bfb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://17c2-35-221-215-83.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [391]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [391]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}